{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [The Gauss Markov Model](#The-Gauss-Markov-Model)\n",
    " - [The OLS Estimator](#The-OLS-Estimator)\n",
    " - [OLS Residual Properties](#OLS-Residual-Properties)\n",
    " - [Finite Sample Properties of the OLS Estimator](#Finite-Sample-Properties-of-the-OLS-Estimator)\n",
    " - [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gauss Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical model for regression data is the **Gauss Markov Model** if each of its distributions satisfies the conditions (1)-(4): linearity, strict exogeneity, no multicollinearity, and spherical error variance. The **Extended Gauss Markov Model** also satisfies assumption (5).\n",
    "\n",
    "1. **Linearity**: a statistical model $\\mathcal{F}$ over data $\\mathcal{D}$ satisfies linearity if for each element of $\\mathcal{F}$, the data can be decomposed in\n",
    "\t$$\n",
    "\t\\begin{aligned}\n",
    "\t  y_ i &= \\beta_ 1 x _ {i1} + \\dots + \\beta_ k x _ {ik} + \\varepsilon_ i = x_ i'\\beta + \\varepsilon_ i \\\\\n",
    "\t  \\underset{n \\times 1}{\\vphantom{\\beta_ \\beta} y} &= \\underset{n \\times k}{\\vphantom{\\beta}X} \\cdot \\underset{k \\times 1}{\\beta} + \\underset{n \\times 1}{\\vphantom{\\beta}\\varepsilon}\n",
    "  \\end{aligned}\n",
    "\t$$\n",
    "2. **Strict Exogeneity**: $\\mathbb E [\\varepsilon_i|x_1, \\dots, x_n] = 0, \\forall i$. \n",
    "3. **No Multicollinerity**: $\\mathbb E_n [x_i x_i']$ is strictly positive definite almost surely. Equivalent to require $rank(X)=k$ with probability $p \\to 1$. \n",
    "\tIntuition: no regressor is a linear combination of other regressors.\n",
    "4. **Spherical Error Variance**: \n",
    "  -$\\mathbb E[\\varepsilon_i^2 | x] = \\sigma^2 > 0, \\ \\forall i$\n",
    "  -$\\mathbb E [\\varepsilon_i \\varepsilon_j |x ] = 0, \\ \\forall$ $1 \\leq i < j \\leq n$\n",
    "5. (Extended GM model) **Normal error term**: $\\varepsilon|X \\sim N(0, \\sigma^2 I_n)$ and $\\varepsilon \\perp X$.\n",
    "\n",
    "Note that by (2) and (4) you get **homoskedasticity**: \n",
    "$$\n",
    "Var(\\varepsilon_i|x) = \\mathbb E[\\varepsilon_i^2|x]- \\mathbb E[\\varepsilon_i|x]^2 = \\sigma^2 I \\qquad \\forall i\n",
    "$$\n",
    "\n",
    "> Implications:\n",
    ">\n",
    "- Strict exogeneity is not restrictive since it is sufficient to include a constant in the regression to enforce it\n",
    "$$\n",
    "\ty_i = \\alpha + x_i'\\beta + (\\varepsilon_i - \\alpha) \\quad \\Rightarrow \\quad \\mathbb E[\\varepsilon_i] = \\mathbb E_x [ \\mathbb E[ \\varepsilon_i | x]] = 0\n",
    "$$\n",
    "- This implies $\\mathbb E[x _ {jk} \\varepsilon_i ] = 0$ by the LIE. \n",
    "- These two conditions together imply $Cov (x _ {jk} \\varepsilon_i ) = 0$.\n",
    "\n",
    "\n",
    "\n",
    "A map $\\Pi: V \\to V$ is a **projection** if $\\Pi \\circ \\Pi = \\Pi$.\n",
    "\n",
    "The Gauss Markov Model assumes that the **conditional expectation function (CEF)** $f(X) = \\mathbb E[Y|X]$ and the **linear projection** $g(X) = X \\beta$ coincide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Set seed\n",
    "rng(123)\n",
    "\n",
    "% Set the number of observations\n",
    "n = 100;\n",
    "\n",
    "% Set the dimension of X\n",
    "k = 2;\n",
    "\n",
    "% Draw a sample of explanatory variables\n",
    "X = rand(n, k);\n",
    "\n",
    "% Draw the error term\n",
    "sigma = 1;\n",
    "e = randn(n,1)*sqrt(sigma);\n",
    "\n",
    "% Set the parameters\n",
    "b = [2; -1];\n",
    "\n",
    "% Calculate the dependent variable\n",
    "y = X*b + e;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The OLS Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sum of squared residuals (SSR)** is given by\n",
    "$$\n",
    "\tQ_n (\\beta) \\equiv   \\frac{1}{n} \\sum _ {i=1}^n \\left( y_i - x_i'\\beta \\right)^2 = \\frac{1}{n} (y - X\\beta)' (y - X \\beta)\n",
    "$$\n",
    "\n",
    "Consider a dataset $\\mathcal{D}$ and define $Q_n(\\beta) = \\mathbb E_n[(y_i - x_i'\\beta )^2 ]$. Then the **ordinary least squares (OLS)** estimator $\\hat \\beta _ {OLS}$ is the value of $\\beta$ that minimizes $Q_n(\\beta)$.\n",
    "\n",
    "When we can write $D = (y, X)$ in matrix form, then \n",
    "$$\n",
    "  \\hat \\beta _ {OLS} = \\arg \\min_\\beta \\frac{1}{n} (y - X \\beta)' (y - X\\beta)\n",
    "$$\n",
    "\n",
    "**Theorem**: \n",
    "Under the assumption that $X$ has full rank, the OLS estimator is unique and it is determined by the normal equations. More explicitly, $\\hat \\beta$ is the OLS estimate precisely when $X'X \\hat \\beta = X'y$.\n",
    "\n",
    "\n",
    "**Proof**: \n",
    "Taking the FOC:\n",
    "$$\n",
    "\t\\frac{\\partial Q_n (\\beta)}{\\partial \\beta} = -\\frac{2}{n} X' y  + \\frac{2}{n} X'X\\beta = 0 \\quad \\Leftrightarrow \\quad X'X \\beta = X'y\n",
    "$$\n",
    "Since $(X'X)^{-1}$ exists by assumption, \n",
    "\n",
    "Finally, $\\frac{\\partial^2 Q_n (\\beta)}{\\partial \\beta \\partial \\beta'} = X'X/n$ is positive definite since $X'X$ is positive semi-definite and $(X'X)^{-1}$ exists because $X$ is full rank. Therefore, $Q_n(\\beta)$ minimized at $\\hat \\beta_n$.\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "The  $k$ equations $X'X \\hat \\beta = X'y$ are called **normal equations**.\n",
    "\n",
    "We can now define the following objects:\n",
    "\n",
    "- Fitted coefficient: $\\hat \\beta _ {OLS} = (X'X)^{-1} X'y = \\mathbb E_n [x_i x_i'] \\mathbb E_n [x_i y_i]$\n",
    "- Fitted residual: $\\hat \\varepsilon_i = y_i - x_i'\\hat \\beta$\n",
    "- Fitted value: $\\hat y_i = x_i' \\hat \\beta$\n",
    "- Predicted coefficient: $\\hat \\beta _ {-i}  = \\mathbb E_n [x _ {-i} x' _ {-i}] \\mathbb E_n [x _ {-i} y _ {-i}]$\n",
    "- Prediction error: $\\hat \\varepsilon _ {-i} = y_i - x_i'\\hat \\beta _ {-i}$\n",
    "- Predicted value: $\\hat y_i = x_i' \\hat \\beta _ {-i}$\n",
    "\n",
    "> Notes on the orthogonality conditions:\n",
    ">\t\n",
    "- The normal equations are equivalent to the moment condition $\\mathbb E_n [x_i \\varepsilon_i]= 0$.\n",
    "- The algebraic result $\\mathbb E_n [x_i \\hat \\varepsilon_i]= 0$  is called **ortogonality property** of the OLS residual $\\hat \\varepsilon_i$.\n",
    "- If we have included a constant in the regression, $\\mathbb E_n [\\hat \\varepsilon_i] = 0$.\n",
    "- $\\mathbb E \\Big[\\mathbb E_n [x_i \\varepsilon_i ] \\Big] = 0$ by strict exogeneity (assumed in GM), but $\\mathbb E_n [x_i \\varepsilon_i]  \\ne \\mathbb E [x_i \\varepsilon_i] = 0$. This is why $\\hat \\beta _ {OLS}$ is just an estimate of $\\beta_0$.\n",
    "- Calculating OLS is like replacing the $j$ equations $\\mathbb E [x _ {ij} \\varepsilon_i] = 0$ $\\forall j$ with $\\mathbb E_n [x _ {ij} \\varepsilon_i] = 0$ $\\forall j$ and forcing them to hold (remindful of GMM).\n",
    "\n",
    "\n",
    "The **projection matrix** is given by $P = X(X'X)^{-1} X'$. It has the following properties:\n",
    "\t\n",
    "- $PX = X$\n",
    "- $P \\hat \\varepsilon = 0 \\quad$ ($P$, $\\varepsilon$ orthogonal)\n",
    "- $P y = X(X'X)^{-1} X'y = X\\hat \\beta = \\hat y$\n",
    "- Symmetric: $P=P'$, Idempotent: $PP = P$\n",
    "- $tr(P) = tr( X(X'X)^{-1} X') = tr( X'X(X'X)^{-1}) = tr(I_k) = k$\n",
    "- Its diagonal elements are $h_{ii} = x_i (X'X)^{-1} x_i'$ and are called **leverage**. \n",
    "\n",
    "> $h _ {ii} \\in [0,1]$ is a normalized length of the observed regressor vector $x_i$. In the OLS regression framework it captures the relative influence of observation $i$ on the estimated coefficient. Note that $\\sum _ n h_{ii} = k$.\n",
    "\t\n",
    "\n",
    "The **annihilator matrix** is given by $M = I_n - P$. It has the following properties:\n",
    "\t\n",
    "- $MX = 0  \\quad$ ($M$, $X$ orthogonal)\n",
    "- $M \\hat \\varepsilon = \\hat \\varepsilon$\n",
    "- $M y = \\hat \\varepsilon$\n",
    "- Symmetric: $M=M'$, idempotent: $MM = M$\n",
    "- $tr(M) = n - k$\n",
    "- Its diagonal elements are $1 - h_{ii} \\in [0,1]$\n",
    "\t\n",
    "> Then we can equivalently write $\\hat y$ (defined by stacking $\\hat y_i$ into a vector) as $\\hat y = Py$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "b_hat =\n",
      "\n",
      "    1.9020\n",
      "   -0.9305\n",
      "\n",
      "\n",
      "b_alternative =\n",
      "\n",
      "    2.1525\n",
      "   -0.7384\n",
      "\n",
      "\n",
      "b_hat_1 =\n",
      "\n",
      "    2.7498\n",
      "    2.1525\n",
      "   -0.7384\n",
      "\n",
      "\n",
      "b_alternative =\n",
      "\n",
      "    2.1525\n",
      "   -0.7384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% Estimate beta\n",
    "b_hat = inv(X'*X)*(X'*y) % = 1.9020, -0.9305\n",
    "\n",
    "% Equivalent but faster formulation\n",
    "b_hat = (X'*X)\\(X'*y);\n",
    "\n",
    "% Even faster (but less intuitive) formulation\n",
    "b_hat = X\\y;\n",
    "\n",
    "% Note that is generally not equivalent to Var(X)^-1 * Cov(X,y)...\n",
    "Var_X = cov(X);\n",
    "Cov_Xy = n/(n-1) * (mean(X .* y) - mean(X).*mean(y));\n",
    "b_alternative = inv(Var_X) * Cov_Xy' % = 2.1525, -0.7384\n",
    "\n",
    "% ...unless you include a constant\n",
    "a = 3;\n",
    "y = a + X*b + e;\n",
    "b_hat_1 = [ones(n,1), X]\\y % = 2.1525, -0.7384\n",
    "Var_X = cov(X);\n",
    "Cov_Xy = n/(n-1) * (mean(X .* y) - mean(X).*mean(y));\n",
    "b_alternative = inv(Var_X) * Cov_Xy' % = 2.1525, -0.7384\n",
    "\n",
    "% Predicted y\n",
    "y_hat = X*b_hat;\n",
    "\n",
    "% Residuals\n",
    "e_hat = y - X*b_hat;\n",
    "\n",
    "% Projection matrix\n",
    "P = X*inv(X'*X)*X';\n",
    "\n",
    "% Annihilator matrix\n",
    "M = eye(n) - P;\n",
    "\n",
    "% Leverage\n",
    "h = diag(P);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS Residual Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is **homoskedastic** if $\\mathbb E [\\varepsilon^2 | x] = \\sigma^2$ does not depend on $x$.\n",
    "$$\n",
    "\tVar(\\varepsilon) = I \\sigma^2 = \\begin{bmatrix}\n",
    "\t\\sigma^2 & \\dots & 0 \\\\\\\\\\\n",
    "\t\\vdots & \\ddots & \\vdots \\\\\n",
    "\t0 & \\dots & \\sigma^2\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The error is **heteroskedastic** if $\\mathbb E [\\varepsilon^2 | x] = \\sigma^2(x)$ does depend on $x$.\n",
    "$$\n",
    "\tVar(\\varepsilon) = I \\sigma_i^2 = \n",
    "\t\\begin{bmatrix}\n",
    "\t\\sigma_1^2 & \\dots & 0 \\\\\n",
    "\t\\vdots & \\ddots & \\vdots \\\\\n",
    "\t0 & \\dots & \\sigma_n^2 \n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The OLS **residual variance** can be an object of interest even in a heteroskedastic regression. Its method of moments estimator is given by\n",
    "$$\n",
    "  \\hat \\sigma^2 = \\frac{1}{n} \\sum _ {i=1}^n \\hat \\varepsilon_i^2\n",
    "$$\n",
    "\n",
    "> Note that $\\hat \\sigma^2$ can be rewritten as \n",
    "$$\n",
    "  \\hat \\sigma^2 = \\frac{1}{n} \\varepsilon' M' M \\varepsilon = \\frac{1}{n} tr(\\varepsilon' M \\varepsilon) = \\frac{1}{n} tr(M \\varepsilon' \\varepsilon)\n",
    "$$\n",
    "\n",
    "However, the method of moments estimator is a biesed estimator. In fact\n",
    "$$\n",
    "  \\mathbb E[\\hat \\sigma^2 | X] = \\frac{1}{n} \\mathbb E [ tr(M \\varepsilon' \\varepsilon) | X] =  \\frac{1}{n} tr( M\\mathbb E[\\varepsilon' \\varepsilon |X]) = \\frac{1}{n} \\sum _ {i=1}^n (1-h_{ii}) \\sigma^2_i\n",
    "$$\n",
    "\n",
    "Under conditional homoskedasticity, the above expression simplifies to\n",
    "$$\n",
    "  \\mathbb E[\\hat \\sigma^2 | X] = \\frac{1}{n} tr(M) \\sigma^2 = \\frac{n-k}{n} \\sigma^2\n",
    "$$\n",
    "\n",
    "The OLS **residual sample variance** is denoted by $s^2$ and is given by\n",
    "$$\n",
    "  s^2 = \\frac{SSR}{n-k} = \\frac{\\hat \\varepsilon'\\hat \\varepsilon}{n-k} = \\frac{1}{n-k}\\sum _ {i=1}^n \\hat \\varepsilon_i^2\n",
    "$$\n",
    "Furthermore, the square root of $s^2$, denoted $s$, is called the standard error of the regression (SER) or the standard error of the equation (SEE).  Not to be confused with other notions of standard error to be defined later in the course.\n",
    "\n",
    "> The sum of squared residuals can be rewritten as: $SSR = \\hat \\varepsilon' \\hat \\varepsilon = \\varepsilon' M \\varepsilon$.\n",
    "\n",
    "The OLS residual sample variance is an unbiased estimator of the error variance $\\sigma^2$.\n",
    "\n",
    "Another unbiased estimator of $\\sigma^2$ is given by\n",
    "$$\n",
    "  \\bar \\sigma^2 = \\frac{1}{n} \\sum _ {i=1}^n (1-h_{ii})^{-1} \\hat \\varepsilon_i^2\n",
    "$$\n",
    "\n",
    "One measure of the variability of the dependent variable $y_i$ is the sum of squares $\\sum _ {i=1}^n y_i^2 = y'y$.  There is a decomposition:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\ty'y &= (\\hat y + e)' (\\hat y + \\hat \\varepsilon) \\\\\n",
    "\t&= \\hat y' \\hat y + 2 \\hat y' \\hat \\varepsilon + \\hat \\varepsilon' \\hat \\varepsilon e \\\\\n",
    "\t&= \\hat y' \\hat y + 2 b'X'\\hat \\varepsilon + \\hat \\varepsilon' \\hat \\varepsilon \\ \\ (\\text{since} \\ \\hat y = Xb) \\\\\n",
    "\t&= \\hat y' \\hat y + \\hat \\varepsilon'\\hat \\varepsilon \\ \\ (\\text{since} \\ X'\\hat \\varepsilon =0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The **uncentered** $\\mathbf{R^2}$ is defined as:\n",
    "$$\n",
    "\tR^2 _ {uc} \\equiv 1 - \\frac{\\hat \\varepsilon'\\hat \\varepsilon}{y'y} = 1 - \\frac{\\mathbb E_n[\\hat \\varepsilon_i^2]}{\\mathbb E_n[y_i^2]} = \\frac{ \\mathbb E [\\hat y_i^2]}{ \\mathbb E [y_i^2]}\n",
    "$$\n",
    "\n",
    "A more natural measure of variability is the sum of centered squares $\\sum _ {i=1}^n (y_i - \\bar y)^2,$ where $\\bar y := \\frac{1}{n}\\sum _ {i=1}^n y_i$. If the regressors include a constant, it can be decomposed as \n",
    "$$\n",
    "  \\sum _ {i=1}^n (y_i - \\bar y)^2 = \\sum _ {i=1}^n (\\hat y_i - \\bar y)^2 + \\sum _ {i=1}^n \\hat \\varepsilon_i^2\n",
    "$$\n",
    "\n",
    "The **coefficient of determination**, $\\mathbf{R^2}$, is defined as \n",
    "$$\n",
    "\tR^2 \\equiv 1 - \\frac{\\sum _ {i=1}^n \\hat \\varepsilon_i^2}{\\sum _ {i=1}^n (y_i - \\bar y)^2 }= \\frac{  \\sum _ {i=1}^n (\\hat y_i - \\bar y)^2 } { \\sum _ {i=1}^n (y_i - \\bar y)^2} = \\frac{\\mathbb E_n[(\\hat y_i - \\bar y)^2]}{\\mathbb E_n[(y_i - \\bar y)^2]}\n",
    "$$\n",
    "\n",
    "> Always use the centered $R^2$ unless you really know what you are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Biased variance estimator \n",
    "sigma_hat = e_hat'*e_hat / n;\n",
    "\n",
    "% Unbiased estimator 1\n",
    "sigma_hat_2 = e_hat'*e_hat / (n-k);\n",
    "\n",
    "% Unbiased estimator 2\n",
    "sigma_hat_3 = mean( e_hat.^2 ./ (1-h) );\n",
    "\n",
    "% R squared - uncentered\n",
    "R2_uc = (y_hat'*y_hat)/ (y'*y);\n",
    "\n",
    "% R squared\n",
    "y_bar = mean(y);\n",
    "R2 = ((y_hat-y_bar)'*(y_hat-y_bar))/ ((y-y_bar)'*(y-y_bar));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Sample Properties of the OLS Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**:\n",
    "Under the GM assumptions (1)-(3), the OLS estimator is **conditionally unbiased**, i.e. the distribution of $\\hat \\beta _ {OLS}$ is centered at $\\beta_0$: $\\mathbb E [\\hat \\beta | X] = \\beta_0$.\n",
    "\n",
    "**Proof**: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\mathbb E [\\hat \\beta  | X] &= \\mathbb E [ (X'X)^{-1} X'y | X] = \\\\\n",
    "\t&= (X'X)^{-1} X ' \\mathbb E  [y | X] = \\\\\n",
    "\t&= (X'X)^{-1} X' \\mathbb E  [X \\beta + \\varepsilon | X] = \\\\\n",
    "\t&= (X'X)^{-1} X'X \\beta + (X'X)^{-1} X' \\mathbb E  [\\varepsilon | X] = \\\\\n",
    "\t&= \\beta \n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\\tag*{$\\blacksquare$}$$\t\n",
    "\n",
    "**Theorem**:\n",
    "Under the GM assumptions (1)-(3), $Var(\\hat \\beta |X) = \\sigma^2 (X'X)^{-1}$.\n",
    "\n",
    "**Proof**: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tVar(\\hat \\beta |X) &= Var( (X'X)^{-1} X'y|X) = \\\\\n",
    "\t&= ((X'X)^{-1} X' ) Var(y|X) ((X'X)^{-1} X' )' = \\\\\n",
    "\t&= ((X'X)^{-1} X' ) Var(X\\beta + \\varepsilon|X) ((X'X)^{-1} X' )' = \\\\\n",
    "\t&= ((X'X)^{-1} X' ) Var(\\varepsilon|X) ((X'X)^{-1} X' )' = \\\\\n",
    "\t&= ((X'X)^{-1} X' ) \\sigma^2 I ((X'X)^{-1} X' )' =  \\\\\n",
    "\t&= \\sigma^2 (X'X)^{-1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "Higher correlation of the $X$ implies higher variance of the OLS estimator. \n",
    "\n",
    "> Intuition: individual observations carry less information. You are exploring a smaller region of the $X$ space.\n",
    "\n",
    "**Theorem**:\n",
    "Under the GM assumptions (1)-(3), $Cov (\\hat \\beta, \\hat \\varepsilon ) = 0$.\n",
    "\n",
    "**Theorem**:\n",
    "Under the GM assumptions (1)-(3), $\\hat \\beta _ {OLS}$ is the best (most efficient) linear, unbiased estimator (**BLUE**), i.e., for any unbiased linear estimator $b$: $Var (b|X) \\geq Var (\\hat \\beta |X)$.\n",
    "\n",
    "**Proof**:  \n",
    "Consider four steps:\n",
    "\n",
    "1. Define three objects: (i) $b= Cy$, (ii) $A = (X'X)^{-1} X'$ such that $\\hat \\beta = A y$, and (iii) $D = C-A$. \n",
    "2. Decompose $b$ as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tb &= (D + A) y = \\\\\n",
    "\t&=  Dy + Ay = \\\\\\\n",
    "\t&= D (X\\beta + \\varepsilon) + \\hat \\beta = \\\\\n",
    "\t&= DX\\beta + D \\varepsilon + \\hat \\beta\n",
    "\\end{aligned}\n",
    "$$\n",
    "3. By assumption, $b$ must be unbiased:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\mathbb E [b|X] &= \\mathbb E [D(X\\beta + \\varepsilon) + Ay |X] = \\\\\n",
    "\t&= \\mathbb E [DX\\beta|X] + \\mathbb E [D\\varepsilon |X] + \\mathbb E [\\hat \\beta |X] = \\\\\n",
    "\t&= DX\\beta + D \\mathbb E [\\varepsilon |X] +\\beta \\\\\\\n",
    "\t&= DX\\beta + \\beta\n",
    "\\end{aligned}\n",
    "$$\n",
    "  Hence, it must be that $DX = 0$\n",
    "4.  We know by (2)-(3) that $b = D \\varepsilon + \\hat \\beta$. We can now calculate its variance.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\tVar (b|X) &= Var (\\hat \\beta + D\\varepsilon|X) = \\\\\n",
    "\t&= Var (Ay + D\\varepsilon|X) = \\\\\n",
    "\t&= Var (AX\\beta + (D + A)\\varepsilon|X) = \\\\\n",
    "\t&= Var((D+A)\\varepsilon |X) = \\\\\n",
    "\t&= (D+A)\\sigma^2 I (D+A)' = \\\\\n",
    "\t&= \\sigma^2 I (DD' + AA' + DA' + AD') = \\\\\n",
    "\t&= \\sigma^2 I (DD' + AA') \\geq \\\\\n",
    "\t&\\geq \\sigma^2 AA'= \\\\\n",
    "\t&= \\sigma^2 (X'X)^{-1} = \\\\\n",
    "\t&= Var (\\hat \\beta|X)\n",
    "\\end{aligned}\n",
    "$$\n",
    "since $DA'= AD' = 0$,  $DX = 0$ and $AA' = (X'X)^{-1}$.\n",
    "$$\\tag*{$\\blacksquare$}$$\n",
    "\n",
    "> $Var(b | X) \\geq Var (\\hat{\\beta} | X)$ is meant in a positive definite sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Ideal variance of the OLS estimator\n",
    "var_b = sigma*inv(X'*X);\n",
    "\n",
    "% Standard errors\n",
    "std_b = sqrt(diag(var_b));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kozbur (2019). PhD Econometrics - Lecture Notes.\n",
    "- Hansen (2019). \"*Econometrics*\".\n",
    "- Wooldridge (2010). \"*Econometric Analysis of Cross Section and Panel Data*\". \n",
    "- Greene (2006). \"*Econometric Analysis*\". \n",
    "- Hayiashi (2000). \"*Econometrics*\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.16.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
